{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "query_data = load_dataset(\"princeton-nlp/LitSearch\", \"query\", split=\"full\")\n",
    "corpus_clean_data = load_dataset(\"princeton-nlp/LitSearch\", \"corpus_clean\", split=\"full\")\n",
    "corpus_s2orc_data = load_dataset(\"princeton-nlp/LitSearch\", \"corpus_s2orc\", split=\"full\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from eval.retrieval.bm25 import BM25\n",
    "from utils import utils\n",
    "from eval.retrieval.kv_store import KVStore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "import argparse\n",
    "\n",
    "\n",
    "def get_index_name(args: argparse.Namespace) -> str:\n",
    "    return os.path.basename(args.dataset_path) + \".\" + args.key\n",
    "\n",
    "def create_index(args: argparse.Namespace) -> KVStore:\n",
    "    index_name = get_index_name(args)\n",
    "\n",
    "    if args.index_type == \"bm25\":\n",
    "        from eval.retrieval.bm25 import BM25\n",
    "        index = BM25(index_name)\n",
    "    elif args.index_type == \"instructor\":\n",
    "        from eval.retrieval.instructor import Instructor\n",
    "        if args.key == \"title_abstract\":\n",
    "            query_instruction = \"Represent the research question for retrieving relevant research paper abstracts:\"\n",
    "            key_instruction = \"Represent the title and abstract of the research paper for retrieval:\"\n",
    "        elif args.key == \"full_paper\":\n",
    "            query_instruction = \"Represent the research question for retrieving relevant research papers:\"\n",
    "            key_instruction = \"Represent the research paper for retrieval:\"\n",
    "        elif args.key == \"paragraphs\":\n",
    "            query_instruction = \"Represent the research question for retrieving passages from relevant research papers:\"\n",
    "            key_instruction = \"Represent the passage from the research paper for retrieval:\"\n",
    "        else:\n",
    "            raise ValueError(\"Invalid key\")\n",
    "        index = Instructor(index_name, key_instruction, query_instruction)\n",
    "    elif args.index_type == \"e5\":\n",
    "        from eval.retrieval.e5 import E5\n",
    "        index = E5(index_name)\n",
    "    elif args.index_type == \"gtr\":\n",
    "        from eval.retrieval.gtr import GTR\n",
    "        index = GTR(index_name)\n",
    "    elif args.index_type == \"grit\":\n",
    "        from eval.retrieval.grit import GRIT\n",
    "        if args.key == \"title_abstract\":\n",
    "            raw_instruction = \"Given a research query, retrieve the title and abstract of the relevant research paper\"\n",
    "        elif args.key == \"full_paper\":\n",
    "            raw_instruction = \"Given a research query, retrieve the relevant research paper\"\n",
    "        elif args.key == \"paragraphs\":\n",
    "            raw_instruction = \"Given a research query, retrieve the passage from the relevant research paper\"\n",
    "        else:\n",
    "            raise ValueError(\"Invalid key\")\n",
    "        index = GRIT(index_name, raw_instruction)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid index type\")\n",
    "    return index\n",
    "\n",
    "def create_kv_pairs(data: List[dict], key: str) -> dict:\n",
    "    if key == \"title_abstract\":\n",
    "        kv_pairs = {utils.get_clean_title_abstract(record): utils.get_clean_corpusid(record) for record in data}\n",
    "    elif key == \"full_paper\":\n",
    "        kv_pairs = {utils.get_clean_full_paper(record): utils.get_clean_corpusid(record) for record in data}\n",
    "    elif key == \"paragraphs\":\n",
    "        kv_pairs = {}\n",
    "        for record in data:\n",
    "            corpusid = utils.get_clean_corpusid(record)\n",
    "            paragraphs = utils.get_clean_paragraphs(record)\n",
    "            for paragraph_idx, paragraph in enumerate(paragraphs):\n",
    "                kv_pairs[paragraph] = (corpusid, paragraph_idx)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid key\")\n",
    "    return kv_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\demo123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\demo123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "Creating LitSearch.title_abstract index: 100%|██████████| 57657/57657 [00:00<00:00, 3217035.41it/s]\n",
      "100%|██████████| 57657/57657 [01:33<00:00, 613.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving index to retrieval_indices\\LitSearch.title_abstract.bm25\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = argparse.Namespace(\n",
    "    index_type=\"bm25\",  # Simulate the --index_type argument\n",
    "    key=\"title_abstract\",  # Simulate the --key argument\n",
    "    dataset_path=\"princeton-nlp/LitSearch\",  # Default value (or you can customize)\n",
    "    index_root_dir=\"retrieval_indices\"  # Default value (or you can customize)\n",
    ")\n",
    "\n",
    "\n",
    "corpus_data = datasets.load_dataset(args.dataset_path, \"corpus_clean\", split=\"full\")\n",
    "index = create_index(args)\n",
    "kv_pairs = create_kv_pairs(corpus_data, args.key)\n",
    "index.create_index(kv_pairs)\n",
    "\n",
    "index_name = get_index_name(args)\n",
    "index.save(args.index_root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "from utils import utils\n",
    "from eval.retrieval.kv_store import KVStore\n",
    "def load_index(index_path: str) -> KVStore:\n",
    "    index_type = os.path.basename(index_path).split(\".\")[-1]\n",
    "    if index_type == \"bm25\":\n",
    "        from eval.retrieval.bm25 import BM25\n",
    "        index = BM25(None).load(index_path)\n",
    "    elif index_type == \"instructor\":\n",
    "        from eval.retrieval.instructor import Instructor\n",
    "        index = Instructor(None, None, None).load(index_path)\n",
    "    elif index_type == \"e5\":\n",
    "        from eval.retrieval.e5 import E5\n",
    "        index = E5(None).load(index_path)\n",
    "    elif index_type == \"gtr\":\n",
    "        from eval.retrieval.gtr import GTR\n",
    "        index = GTR(None).load(index_path)\n",
    "    elif index_type == \"grit\":\n",
    "        from eval.retrieval.grit import GRIT\n",
    "        index = GRIT(None, None).load(index_path)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid index type\")\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(\n",
    "    index_name=\"LitSearch.title_abstract.bm25\",  # Simulate the --index_name argument\n",
    "    top_k=200,  # Simulate the --top_k argument with a default value\n",
    "    retrieval_results_root_dir=\"results/retrieval\",  # Default value\n",
    "    index_root_dir=\"retrieval_indices\",  # Default value\n",
    "    dataset_path=\"princeton-nlp/LitSearch\"  # Default value\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\demo123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\demo123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index from retrieval_indices\\LitSearch.title_abstract.bm25...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 597/597 [02:14<00:00,  4.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 597 records to results/retrieval\\LitSearch.title_abstract.bm25.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "index = load_index(os.path.join(args.index_root_dir, args.index_name))\n",
    "query_set = [query for query in datasets.load_dataset(args.dataset_path, \"query\", split=\"full\")]\n",
    "for query in tqdm(query_set):\n",
    "    query_text = query[\"query\"]\n",
    "    top_k = index.query(query_text, args.top_k)\n",
    "    query[\"retrieved\"] = top_k\n",
    "\n",
    "os.makedirs(args.retrieval_results_root_dir, exist_ok=True)\n",
    "output_path = os.path.join(args.retrieval_results_root_dir, f\"{args.index_name}.jsonl\")\n",
    "utils.write_json(query_set, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
